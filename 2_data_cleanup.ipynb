{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72568c9c",
   "metadata": {},
   "source": [
    "Data cleanup\n",
    "---\n",
    "\n",
    "This notebook takes the results from the data acquisition stage and merges them into a pair of files, one for tweet data and one for user data.\n",
    "\n",
    "The cleaned data is saved in the [merged_data](./merged_data/)\n",
    "\n",
    "This creates a couple large files `/merged_data/merged_tweets.csv` and `/merged_data/merged_users.csv`. These should be ran on a local machine and `.gitignore`d. They are not included in this repo.\n",
    "\n",
    "The tweets and user data are merged in the [following notebook featuring EDA](./3_EDA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb03e4b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445a1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "#######################################################\n",
    "# change this when importing newly gathered tweet data\n",
    "#######################################################\n",
    "max_csv_number = 193"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7410985",
   "metadata": {},
   "source": [
    "## Merging and saving tweet data\n",
    "\n",
    "Go through each tweets or users csv in `raw_data`, concatenate them, and save the result.\n",
    "\n",
    "\n",
    "### get_raw_data\n",
    "Reads a single csv and returns a dataframe.\n",
    "\n",
    "### get_merged_data\n",
    "Repeatedly calls `get_raw_data` and merges all of the dataframes into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9505620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_path = './merged_data/'\n",
    "raw_data_path = './raw_data/'\n",
    "\n",
    "#loads the data in a csv and returns a dataframe with it.\n",
    "# can get tweet data or user data.\n",
    "def get_raw_data(kind = 'tweets', number = 0):\n",
    "    new_df = pd.read_csv(f'{raw_data_path}{kind}_{number}.csv')\n",
    "    new_df.set_index('id', inplace = True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# repeatedly call the previous, tacking each onto the end of a new dataframe,\n",
    "# and return the dataframe\n",
    "def get_merged_data(kind = 'tweets', numbers = [0,1]):\n",
    "    #make a new blank df\n",
    "    new_df = pd.DataFrame()\n",
    "    #grab each file and tack data onto the end of the dataframe\n",
    "    for number in numbers:\n",
    "        this_df = get_raw_data(kind = kind, number = number)\n",
    "        if kind == 'tweets':\n",
    "            new_df = pd.concat([new_df, this_df])\n",
    "        else: #users or users_idfixed\n",
    "            #explicitly making it 'outer' may be unneccesary. Just making sure\n",
    "            # there are no duplicates.\n",
    "            new_df = pd.concat([new_df, this_df], join = 'outer')\n",
    "    #convert created_at string to datetime before returning\n",
    "    if 'created_at' in new_df.columns:\n",
    "        new_df['created_at'] = pd.to_datetime(new_df['created_at'])\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e18524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#perform the merge for all saved tweet csvs\n",
    "tweets_df = get_merged_data(kind = 'tweets', numbers = range(max_csv_number + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d57568a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1521733018293084160    Wordle (ES)  #118 5/6\\n\\nâ¬œâ¬œðŸŸ¨ðŸŸ©â¬œ\\nâ¬œâ¬œðŸŸ¨ðŸŸ©â¬œ\\nâ¬œâ¬œâ¬œðŸŸ©ðŸŸ©\\n...\n",
       "1521733015738662913    Wordle CientÃ­fico #52 5/6\\n\\nâ¬œâ¬œâ¬œâ¬œðŸŸ¨\\nðŸŸ¨â¬œâ¬œâ¬œðŸŸ©\\nâ¬œðŸŸ©â¬œ...\n",
       "1521733011414257664         Wordle 319 4/6\\n\\nðŸŸ¨ðŸŸ¨â¬œâ¬œâ¬œ\\nðŸŸ¨â¬œâ¬œðŸŸ©â¬œ\\nðŸŸ©ðŸŸ¨â¬œðŸŸ©ðŸŸ¨\\nðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©\n",
       "1521733010504445954    Back in the saddle.\\n\\nWordle 319 3/6\\n\\nâ¬œðŸŸ©â¬œðŸŸ¨â¬œ...\n",
       "1521733001981444096    @RwellsWells Same. Are we both spotters? Haha ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine the top of the results.\n",
    "tweets_df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0250d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 386457 entries, 1521733018293084160 to 1521994876677672966\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count   Dtype              \n",
      "---  ------               --------------   -----              \n",
      " 0   conversation_id      386457 non-null  int64              \n",
      " 1   reply_settings       386457 non-null  object             \n",
      " 2   lang                 386457 non-null  object             \n",
      " 3   entities             123247 non-null  object             \n",
      " 4   possibly_sensitive   386457 non-null  bool               \n",
      " 5   source               386457 non-null  object             \n",
      " 6   public_metrics       386457 non-null  object             \n",
      " 7   text                 386457 non-null  object             \n",
      " 8   context_annotations  381900 non-null  object             \n",
      " 9   author_id            386457 non-null  int64              \n",
      " 10  created_at           386457 non-null  datetime64[ns, UTC]\n",
      " 11  referenced_tweets    40081 non-null   object             \n",
      " 12  in_reply_to_user_id  37095 non-null   float64            \n",
      " 13  attachments          4646 non-null    object             \n",
      " 14  geo                  6099 non-null    object             \n",
      " 15  withheld             4 non-null       object             \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(1), int64(2), object(11)\n",
      "memory usage: 47.5+ MB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b13053",
   "metadata": {},
   "source": [
    "Dropping duplicate IDs with the help of [This stack overflow question.](https://stackoverflow.com/questions/13035764/remove-pandas-rows-with-duplicate-indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49aa226e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 335327 entries, 1521733018293084160 to 1521994876677672966\n",
      "Data columns (total 16 columns):\n",
      " #   Column               Non-Null Count   Dtype              \n",
      "---  ------               --------------   -----              \n",
      " 0   conversation_id      335327 non-null  int64              \n",
      " 1   reply_settings       335327 non-null  object             \n",
      " 2   lang                 335327 non-null  object             \n",
      " 3   entities             107512 non-null  object             \n",
      " 4   possibly_sensitive   335327 non-null  bool               \n",
      " 5   source               335327 non-null  object             \n",
      " 6   public_metrics       335327 non-null  object             \n",
      " 7   text                 335327 non-null  object             \n",
      " 8   context_annotations  331475 non-null  object             \n",
      " 9   author_id            335327 non-null  int64              \n",
      " 10  created_at           335327 non-null  datetime64[ns, UTC]\n",
      " 11  referenced_tweets    35202 non-null   object             \n",
      " 12  in_reply_to_user_id  32555 non-null   float64            \n",
      " 13  attachments          4130 non-null    object             \n",
      " 14  geo                  5333 non-null    object             \n",
      " 15  withheld             4 non-null       object             \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(1), int64(2), object(11)\n",
      "memory usage: 41.3+ MB\n"
     ]
    }
   ],
   "source": [
    "tweets_df = tweets_df[~tweets_df.index.duplicated(keep = 'first')]\n",
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04e7d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#save the merged data\n",
    "tweets_df.to_csv(merged_data_path + 'merged_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc029d",
   "metadata": {},
   "source": [
    "Now do the same for the users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d760ec3",
   "metadata": {},
   "source": [
    "## Merging and saving user data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce4df5",
   "metadata": {},
   "source": [
    "### Fixing user data ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fe44f",
   "metadata": {},
   "source": [
    "The user data needs to be edited a bit and resaved. The `Unnamed: 0` column is partly indices 0-99, and the index is id, while later they switch. In fact, the ids are all NaNs later on! \n",
    "\n",
    "It does seem like it's not reliably always exactly the first 100 with these indices, so I will make sure to check appropriately.\n",
    "\n",
    "- for each user_xx.csv,\n",
    "    - pull it in\n",
    "    - check through all non-NaN indices. for each,\n",
    "        - copy the index into the Unnamed: 0 column\n",
    "        - reindex on Unnamed: 0\n",
    "        - verify that all indices are unique, nothing is lost\n",
    "        - save an updated copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d163bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix user indexes.\n",
    "# pulls in `users_{number}.csv, fixes the index so that it is the user id, and saves a new copy.\n",
    "\n",
    "raw_data_path = './raw_data/'\n",
    "old_user_data_path = raw_data_path\n",
    "\n",
    "\n",
    "def fix_user_index(number, extension = '_idfixed'):\n",
    "    this_user_df = pd.read_csv(old_user_data_path + f'users_{number}.csv', index_col=None)\n",
    "    this_user_df['id'] = this_user_df.apply(lambda row: row['Unnamed: 0'] if row.isna()['id'] else row['id'], axis = 1).astype(np.int64)\n",
    "    this_user_df.set_index('id', inplace = True)\n",
    "    this_user_df.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "    #save in the raw data path. do not overwrite the originals.\n",
    "    this_user_df.to_csv(raw_data_path + f'users{extension}_{number}.csv')\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c0d585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fix all of the user data and save new csvs\n",
    "first_users_number = 0\n",
    "last_users_number = max_csv_number\n",
    "for user_number in range(first_users_number, last_users_number + 1):\n",
    "    fix_user_index(user_number)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf925214",
   "metadata": {},
   "source": [
    "### Merging and saving id-fixed user data\n",
    "\n",
    "This procedure is identical as that for the tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccfc41aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "users_df = get_merged_data(kind = 'users_idfixed', numbers = range(max_csv_number+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ef9b93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 383910 entries, 1006110502043471872 to 125443746\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   name               383897 non-null  object \n",
      " 1   public_metrics     383910 non-null  object \n",
      " 2   url                117064 non-null  object \n",
      " 3   description        336144 non-null  object \n",
      " 4   profile_image_url  383910 non-null  object \n",
      " 5   location           272627 non-null  object \n",
      " 6   protected          383910 non-null  bool   \n",
      " 7   username           383910 non-null  object \n",
      " 8   verified           383910 non-null  bool   \n",
      " 9   entities           161563 non-null  object \n",
      " 10  pinned_tweet_id    181222 non-null  float64\n",
      " 11  withheld           4 non-null       object \n",
      "dtypes: bool(2), float64(1), object(9)\n",
      "memory usage: 33.0+ MB\n"
     ]
    }
   ],
   "source": [
    "users_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79cf65b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 147313 entries, 1006110502043471872 to 233970658\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   name               147306 non-null  object \n",
      " 1   public_metrics     147313 non-null  object \n",
      " 2   url                44780 non-null   object \n",
      " 3   description        129085 non-null  object \n",
      " 4   profile_image_url  147313 non-null  object \n",
      " 5   location           105401 non-null  object \n",
      " 6   protected          147313 non-null  bool   \n",
      " 7   username           147313 non-null  object \n",
      " 8   verified           147313 non-null  bool   \n",
      " 9   entities           62081 non-null   object \n",
      " 10  pinned_tweet_id    68630 non-null   float64\n",
      " 11  withheld           1 non-null       object \n",
      "dtypes: bool(2), float64(1), object(9)\n",
      "memory usage: 12.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#drop duplicates. ~... is the list of users to keep, the ones that are NOT duplicates\n",
    "users_df = users_df[~users_df.index.duplicated(keep = 'first')]\n",
    "users_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d3fcf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged user data\n",
    "users_df.to_csv(merged_data_path + 'merged_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be774997",
   "metadata": {},
   "source": [
    "# Next step: EDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
