{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06add69c",
   "metadata": {},
   "source": [
    "Data Acquisition\n",
    "---\n",
    "\n",
    "This notebook shows the functions used to collect and save data using the Twitter API v2. The [original notebook](./data_acquisition/2_data_acquisition.ipynb) that gathered the data is in the data acquisition folder. File paths have been altered for this notebook, but the reader will need to provide their own Twitter Developer bearer token.\n",
    "\n",
    "Data are saved in the [raw_data](./raw_data/) directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d9f24",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97938973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# searching and interpreting\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d8c0d8",
   "metadata": {},
   "source": [
    "---\n",
    "## Define functions for search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1264bc",
   "metadata": {},
   "source": [
    "### format_twitter_search_parameters\n",
    "\n",
    "The search request requires a search query as well as a list of parameters to request in the response. There are many choices for these parameters. The user planning to perform the search should [familiarize themselves with this Twitter API v2 page](https://developer.twitter.com/en/docs/twitter-api/tweets/lookup/introduction).\n",
    "\n",
    "This function accepts a series of parameters and generates a string of the appropriate format for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ae885ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_twitter_search_parameters(tweet_fields = default_tweet_fields,\n",
    "                                     user_fields = default_user_fields,\n",
    "                                     max_results = 10,\n",
    "                                     next_token = '',\n",
    "                                     end_time = '',\n",
    "                                     until_id = 0,\n",
    "                                     since_id = 0\n",
    "                                    ):\n",
    "    #optionally search up to an end time\n",
    "    if end_time != '':\n",
    "        end_time_bit = '&end_time=' + end_time\n",
    "    else:\n",
    "        end_time_bit = end_time\n",
    "    #optionally search after a previous search using its next_token value (from 'meta')\n",
    "    if next_token != '':\n",
    "        next_token_bit = '&next_token=' + next_token\n",
    "    else:\n",
    "        next_token_bit = next_token\n",
    "    #optionally search only before a certain tweet id\n",
    "    if until_id == 0:\n",
    "        until_id_bit = ''\n",
    "    else:\n",
    "        until_id_bit = '&until_id=' + str(until_id)\n",
    "    #optionally search only since a certain tweet id\n",
    "    if since_id == 0:\n",
    "        since_id_bit = ''\n",
    "    else:\n",
    "        since_id_bit = '&since_id=' + str(since_id)\n",
    "    #assemble the string\n",
    "    parameter_string = 'tweet.fields=' + ','.join(tweet_fields) + '&user.fields=' + ','.join(user_fields) + '&max_results='+ str(max_results)+ until_id_bit + since_id_bit + next_token_bit + end_time_bit\n",
    "    return parameter_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c69fd",
   "metadata": {},
   "source": [
    "### replace_next_token\n",
    "\n",
    "Twitter searches stop at 100 tweets maximum, and give a token to send with a following request to read the next page. This function rewrites a twitter search parameter string to include a token for the following page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aefabc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a twitter search parameter string with a next_token bit on it, removes the token, and adds the new one.\n",
    "# returns the new string\n",
    "def replace_next_token(twitter_search_parameters, next_token):\n",
    "    next_token_label = '&next_token='\n",
    "    split_parameters = twitter_search_parameters.split(next_token_label)\n",
    "    new_parameter_string = split_parameters[0] + next_token_label + str(next_token)\n",
    "    return new_parameter_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c8764",
   "metadata": {},
   "source": [
    "## search_twitter\n",
    "\n",
    "This function searches Twitter for a query, interpreting the query identically to using the search bar on the main Twitter page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b853d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_file_path = '../APIKEYS/Twitter.txt'\n",
    "bearer_token_file_path = '../APIKEYS/twitter_bearer_token.txt'\n",
    "with open(bearer_token_file_path, 'r') as bearer_token_file:\n",
    "    my_bearer_token = bearer_token_file.read()\n",
    "\n",
    "#its bad practice to place your bearer token directly into the script (this is just done for illustration purposes)\n",
    "BEARER_TOKEN = my_bearer_token\n",
    "#define search twitter function\n",
    "def search_twitter(query, tweet_fields, bearer_token = BEARER_TOKEN):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&{}\".format(\n",
    "        query, tweet_fields\n",
    "    )\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    print(response.status_code)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6341d8",
   "metadata": {},
   "source": [
    "### print_progress_bar\n",
    "\n",
    "For visualizing a search. Used by `repeat_twitter_search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a6240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress_bar(current, total, bar_length = 100):\n",
    "    clear_output()\n",
    "    print('[' + '='*int(np.ceil(bar_length*current/total)) + '.'*int(np.ceil(bar_length*(total-current)/total)) + ']')\n",
    "    print(f'{100*current/total}%'+ ' '*10 + f'{current}/{total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed9d090",
   "metadata": {},
   "source": [
    "### repeat_twitter_search\n",
    "\n",
    "This repeatedly performs a search of twitter, returning the next token for reading further pages of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7387b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a query and search parameters, formats it and performs a series of searches. returns a dict with\n",
    "# aggregated results and a token for the next page if you desire to continue searching\n",
    "def repeat_twitter_search(search_query, tweet_fields, user_fields, search_size = 100, until_id = 0, since_id = 0, end_time = '', num_searches = 10, first_next_token = ''):\n",
    "    # do a \"priming search\", preparing the dataframe and filling it with first values. the expansions bit is necessary for grabbing the associated user data\n",
    "    search_parameters = format_twitter_search_parameters(tweet_fields = tweet_fields, user_fields = user_fields, max_results = search_size, until_id = until_id, since_id=since_id, end_time = end_time)+ '&expansions=author_id'\n",
    "    # this can start with a next_token already defined.\n",
    "    if first_next_token != '':\n",
    "        search_parameters = search_parameters  + '&next_token=' + first_next_token \n",
    "    search_result = search_twitter(search_query, search_parameters)\n",
    "    tweets_df = pd.DataFrame(search_result['data'])\n",
    "    users_df = pd.DataFrame(search_result['includes']['users'])\n",
    "    tweets_df.set_index('id', inplace = True)\n",
    "    #store the next token for reading the second page of results. the loop ahead grabs successive pages.\n",
    "    next_token = search_result['meta']['next_token']\n",
    "    #edit the parameters to get the next page.\n",
    "    # add in a next_token= to the parameters if it doesn't already have one.\n",
    "    if first_next_token == '':\n",
    "        search_parameters = search_parameters + '&next_token=' + search_result['meta']['next_token']\n",
    "    #loop through the following searches, concatting their results to the dataframe.\n",
    "    for search_number in range(num_searches-1):\n",
    "        print_progress_bar(search_number, num_searches-1)\n",
    "        #edit parameters to get the next page\n",
    "        #expansions is needed to get the user data.\n",
    "        search_parameters = replace_next_token(search_parameters, next_token)\n",
    "        search_result = search_twitter(search_query, search_parameters)\n",
    "        new_tweets_df = pd.DataFrame(search_result['data'])\n",
    "        new_tweets_df.set_index('id', inplace = True)\n",
    "        tweets_df = pd.concat([tweets_df, new_tweets_df])\n",
    "        new_users_df = pd.DataFrame(search_result['includes']['users'])\n",
    "        new_users_df.set_index('id', inplace = True)\n",
    "        users_df = pd.concat([users_df, new_users_df])\n",
    "        next_token = search_result['meta']['next_token']\n",
    "    return tweets_df, users_df, next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e132087",
   "metadata": {},
   "source": [
    "## Gather Data\n",
    "\n",
    "These cells perform successive searches of twitter for a set search query over many hours. The gathered data is saved in a series of CSV files in `./raw_data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5f2894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path to save data\n",
    "raw_data_path = './raw_data/'\n",
    "\n",
    "#define the first suffix for the saved csvs. All data in this repo has a number <200, so this will not interfere.\n",
    "first_csv_number = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5425653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================================================================..........]\n",
      "90.0%          18/20\n",
      "200\n",
      "Wall time: 5h 43min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# gather and save data for a few hours.\n",
    "\n",
    "#set the search query\n",
    "the_search_query = 'ðŸŸ© Wordle'\n",
    "\n",
    "# set this to search since the tweet with this id\n",
    "this_since_id = 1521924000000000000\n",
    "\n",
    "# set this to search until the tweet with this id\n",
    "this_until_id = 1521994876677672966\n",
    "\n",
    "#set length and rate of searches\n",
    "num_hours = 6\n",
    "searches_per_hour = 4\n",
    "\n",
    "#perform an initial search without a subsequent page token.\n",
    "word_search_tweets, word_search_users, word_search_token = repeat_twitter_search(search_query = the_search_query,\n",
    "                                                                                                  tweet_fields = default_tweet_fields,\n",
    "                                                                                                  user_fields= default_user_fields,\n",
    "                                                                                                   search_size=100,\n",
    "                                                                                                   until_id = this_until_id,\n",
    "                                                                                                  since_id = this_since_id,\n",
    "                                                                                                   num_searches=20,\n",
    "                                                                                                   first_next_token='')\n",
    "word_search_tweets.to_csv(raw_data_path + f'tweets_{first_csv_number}.csv')\n",
    "word_search_users.to_csv(raw_data_path+f'users_{first_csv_number}.csv')\n",
    "\n",
    "#perform successive searches\n",
    "for number_search_repeats in range(1, num_hours*searches_per_hour):\n",
    "    print(f'Waiting for search repeat {number_search_repeats}/{num_hours*searches_per_hour}')\n",
    "    sleep((60/searches_per_hour)*60) # (60min/4 = 15min )*(secs/min)\n",
    "    word_search_tweets, word_search_users, word_search_token = repeat_twitter_search(search_query = the_search_query,\n",
    "                                                                                                  tweet_fields = default_tweet_fields,\n",
    "                                                                                                  user_fields= default_user_fields,\n",
    "                                                                                                   search_size=100,\n",
    "                                                                                                   until_id = this_until_id,\n",
    "                                                                                                 since_id = this_since_id,\n",
    "                                                                                                   num_searches=20,\n",
    "                                                                                                   first_next_token=word_search_token)\n",
    "    word_search_tweets.to_csv(raw_data_path + f'tweets_{number_search_repeats + first_csv_number}.csv')\n",
    "    word_search_users.to_csv(raw_data_path + f'users_{number_search_repeats + first_csv_number}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43afc2",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "The next step is to clean this data in preparation for an exploratory data analysis. This cleanup is performed in the following notebook, [Data Cleanup](./2_data_cleanup.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
